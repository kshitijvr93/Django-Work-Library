'''
(1) For a given excel spreadsheet print the header column names.
'''

import sys, os, os.path, platform
import datetime
from collections import OrderedDict

def register_modules():
    platform_name = platform.system().lower()
    if platform_name == 'linux':
        modules_root = '/home/robert/'
        #raise ValueError("MISSING: Enter code here to define modules_root")
    else:
        # assume rvp office pc running windows
        modules_root="C:\\rvp\\"
    sys.path.append('{}git/citrus/modules'.format(modules_root))
    return
register_modules()

print("Using sys.path={}".format(repr(sys.path)))

import etl
# Import slate of databases that podengo can use
from sqlalchemy_tools.podengo_db_engine_by_name import get_db_engine_by_name

#### Sqlalchemy
import datetime
from sqlalchemy import (
  Boolean, create_engine,
  CheckConstraint, Column, Date, DateTime,Float, FLOAT, ForeignKeyConstraint,
  inspect, Integer,
  MetaData, Sequence, String, Table, Text, UniqueConstraint,
  )
from sqlalchemy.schema import CreateTable

import sqlalchemy.sql.sqltypes

#
from pathlib import Path
from etl import html_escape, has_digit, has_upper, make_home_relative_folder
import xlrd, xlwt
from xlwt import easyxf
from xlrd import open_workbook
#
from dataset.dataset_code import SheetDictReader, BookSheetFilter

'''
Given a table name and metadata, create a table configuration with:
(1) a primary key column generated by this method
(2) followed by either a list of columns in argument columns
(3) or, if not given, then use column names to create Text columns for
each of the column names

'''
def sqlalchemy_core_table(
  metadata=None, table_name=None, columns=None, column_names=None):
    me = 'sqlalchemy_core_table'
    # create primary key column
    table_columns = [Column(
     '{}_id'.format(table_name), Integer,
      Sequence('{}_id_seq'.format(table_name)), primary_key=True,)]

    if columns is None:
        for c in column_names:
            print("Using Column name={}",c)
            table_columns.append(Column('{}'.format(c),Text))
    else: #just use columns arg
        table_columns.extend(columns)

    core_table = Table(table_name, MetaData(),*columns);

    print("{}: using configured table {}".format(me,core_table.name))
    return core_table

'''
Just print the table creation ddl
'''

def engine_table_create (metadata=None,table_core=None,engine=None,verbosity=1):

    me = "engine_table_create"
    if table_core is None:
      raise ValueError("{}:Got a table_core value of None".format(me))

    if verbosity > 0:
        print('\n{}:USING-----------------TABLE {}----------------------------\n'
          .format(me,table_core.name))
        print('{}:USING-----------------ENGINE {}--------------------------\n'
          .format(me,engine.name))

    if verbosity > 0:
        # print the ddl to create the table
        print('====== Start CreateTable(table_core.compile(engine) output ===')
        #print(CreateTable(table_core).compile(engine))
        print('====== End CreateTable(table_core.compile(engine) output =====')

    # Create this table in the engine?
    #table.__table__.create(engine, checkfirst=True)
    engine_table = table_core.create(engine, checkfirst=True)
    # Return None for now...just pringing the schema..
    return engine_table

# end engine_table_create
def workbook_columns(workbook_path=None,sqlalchemy_columns=None):
    #initialize database connections for writing/inserting
    workbook = xlrd.open_workbook(workbook_path)
    first_sheet = workbook.sheet_by_index(0)
    if first_sheet is None:
      raise ValueError("Sheet is None")
    # output
    #reader = SheetDictReader(book=workbook,
    reader = BookSheetFilter(book=workbook,
      sheet=first_sheet, row_count_header=1, row_count_values_start=2,
      sqlalchemy_columns=sqlalchemy_columns,verbosity=0)

    for column_name in reader.column_names:
        column_name = column_name.strip().lower().replace(' ', '_')
        print("{}".format(repr(column_name)))

    return reader.column_names

'''
Connect to a database and insert spreadsheet rows to table

<param name='workbook_path'>
File path to an excel workbook to open, and use the first sheet as the
data source.
</param>
<param name='cxs_format'>
A python string format to use to construct the database connection string,
along with other parameter d_format_params.
</param>
<param name='d_format_params'>
Defines the names and values to use to insert into the cxs_format paramater
string, to use to create a database connection.
</param>
<note>
Used to use next default, but keep here for reference
  cxs_format='mysql+mysqldb://{user}:{password}@127.0.0.1:3306/{dbname}',
 </note>
'''

def spreadsheet_to_engine_table(
  workbook_path=None, sheet_index=0,
  od_index_column=None,
  table_core=None, engine_table=None,
  engine=None,
  row_count_header=1, row_count_values_start=2, verbosity=0):

    me = 'spreadsheet_to_engine_table'
    required_args = [
      'workbook_path', 'table_core', 'engine', 'od_index_column' ]

    if not all(required_args):
      msg = "Missing some required args in {}".format(repr(required_args))
      raise ValueError(msg)

    #initialize database connections for writing/inserting

    if verbosity > 1:
        # Might experiment with these later...
        print("+++++++++++= Calling Metadata(engine)")
        metadata = MetaData(engine)
        print("+++++++++++= Calling inspect(engine)")
        inspector = inspect(engine)
        print("+++++++++++= Calling inspect(engine)")
        conn = engine.connect()
        # Warning: this causes hundreds of lines of output in a 5-table database
        print("+++++++++++= Calling metadata.reflect(engine)")
        metadata.reflect(engine)
        print("+++++++++++= Returned from metadata.reflect(engine)")

    if (verbosity > 0):
        print('Connected with conn={} to database to insert into table {}'
          .format(repr(conn),table.name))
        sys.stdout.flush()

    #workbook
    workbook = xlrd.open_workbook(workbook_path)
    # initialize sheet reader for the workbook

    reader = SheetDictReader(
      book=workbook,sheet_index=sheet_index, row_count_header=row_count_header,
      row_count_values_start=row_count_values_start)
    first_sheet = workbook.sheet_by_index(reader.sheet_index)
    print("{}: SheetDictReader with sheet_index {} has column_names={}"
        .format(me, reader.sheet_index, repr(reader.column_names)))
    sys.stdout.flush()

    #Read each spreadsheet row and insert table row
    #based on od_index_column information

    i = 0
    for row in reader:
        i += 1
        if (verbosity > 0 or 1 == 1):
            msg = ("{}:reading ss row {}={}".format(me,i,repr(row)))
            print(msg.encode('utf-8'))

        od_table_column__value = {}

        # Filter out the interesting table_column value pairs for insertion
        # into the table
        for index, column in od_index_column.items():
            # Create entry in filtered dict with key of each interesting
            # output column name paired with its  row's value
            value = row[reader.column_names[index]]

            #Insert logic here -- if the column is FLOAT, and the
            #value is empty string, then insert None (for db value NULL)
            print(
              "Output:index={}, col name={}, col.type={}, type(column.type)={}"
              .format(index, column.name, column.type, type(column.type)))

            if ( type(column.type) == sqlalchemy.sql.sqltypes.Float
               or type(column.type) == sqlalchemy.sql.sqltypes.Integer
               # Add types here as needed later if db row insertion of '' fails
               # in this or that db engine of interest
               ):
                #NOTE: In postgres, do not need this replacement for integer
                # field,  but do for Float and maybe others...
                if value == '':
                    value = None
                    od_table_column__value[column.name] = value
                else:
                    value = float(value)
                    od_table_column__value[column.name] = value
                #print("Got a FLOAT set value={}".format(value))
            elif type(column.type) == sqlalchemy.sql.sqltypes.Integer:
                if value == '':
                    value = None
                    od_table_column__value[column.name] = value
                else:
                    value = float(value)
                    od_table_column__value[column.name] = value
            else:
                od_table_column__value[column.name] = value

            msg = ("index={}, column_name={}, value={}"
              .format(i,index,column.name, value))

            # Try to avoid windows msg: UnicodeEncodeError...
            # on prints to windows console, encode in utf-8
            # It works FINE!
            print(msg.encode('utf-8'))
            #print(msg)
            sys.stdout.flush()

        msg = ("row={}"
          .format(od_table_column__value))
        #engine.execute(engine_table.insert(), od_table_column__value)
        engine.execute(table_core.insert(), od_table_column__value)

        if i % 100 == 0:
           print(i)

#end spreadsheet_to_engine_table(workbook_path=None, table=None, engine=None):
'''
Test linux using postgresql example
Required:
A workbook has been deposited at workbook_path and has at least one excel-known
date column

And the posgresql database engine is up and running and the test table does
not exist (just drop that table before running this)

'''
def xxtest_linux(nick_name=None):
    me = 'test_linux_postgres'

    metadata = MetaData()
    engine_nick_name = nick_name
    my_db_engine = get_db_engine_by_name(engine_nick_name)

    workbook_path = ('/home/robert/Downloads/'
        'lone_cabbage_data_janapr_2017.xlsx')

    table_name = "test_table2"
    # Nick name is used by podengo_db_engine_by_bame() to get
    # the desired engine in which to create the table and insert rows.
    # todo: make it drop the table first, or give option to add new
    # rows if table already extant in the engine/database.

    #engine_nick_name = 'local-silodb'
    #engine_nick_name = 'mysql-marshal1'


    print("Calling workbook_columns()....")
    ss_columns = workbook_columns(workbook_path=workbook_path)

    #normalize spreadsheet column names to db table column names
    # Consider moving this 'replace' stuff into SheetDictReader code
    # maybe as a dictionary param or  even hard-coded there .
    # But just do it explicitly here for now
    d_ss_column__table_column = {}
    for ss_column in ss_columns:
        table_column = column_name_normalize(column_name=ss_column)
        d_ss_column__table_column[ss_column] = table_column

    print(
      "{}: using d_ss_column__table_column={}"
      .format(me,repr(d_ss_column__table_column)))


    table=sqlalchemy_core_table(metadata=metadata, table_name=table_name,
        column_names=d_ss_column__table_column.values(), )

    # select a db engine
    # get_db_engine_by_name is a custom function per user who runs this
    # test for now, with that users creds on linux or windows if windows
    # authentication is not being used on the windows database

    tables = [table]
    # todo:Change to arg of single table instead of list tables
    # create the table if not extand
    creates_run(metadata=metadata,engine=my_db_engine,tables=tables)

    #Add rows to the table from the spreadsheet
    spreadsheet_to_engine_table(workbook_path=workbook_path,
       sheet_index=sheet_index, table=table,
       engine=my_db_engine,od_index_column=od_index_column)

    return
# end test_linux


'''
Set workbook_path to any workbook path on local drive

Required: the test workbook is at workbook_path, defined below.
'''
def spreadsheet_to_table(
  input_workbook_path=None, sheet_index=0,
  od_index_column=None,
  engine_nick_name=None, table_name=None
  ):
    me = 'test_spreadsheet_table'
    required_args =[
      'input_workbook_path',
      'od_index_column',
      'engine_nick_name',
      'table_name',
    ]
    if not all(required_args):
      msg=("Mising some required args: {}".format(repr(required_args)))
      raise ValueError(msg)

    metadata = MetaData()
    my_db_engine = get_db_engine_by_name(name=engine_nick_name)

    #Create the in-memory table_core object.

    table_core = sqlalchemy_core_table(
      table_name=table_name, columns=od_index_column.values())

    # todo: only show the create ddl for the table
    # TODO:From the table_core, create a true database table object

    engine_table = engine_table_create(
      metadata=metadata, engine=my_db_engine, table_core=table_core)

    #Add rows to the table from the spreadsheet

    spreadsheet_to_engine_table(
       workbook_path=input_workbook_path, engine_table=engine_table,
       sheet_index=sheet_index,
       table_core=table_core,
       engine=my_db_engine, od_index_column=od_index_column )

    return
# end test_windows()

def run(env=None):
    me='run()'

    #NOTE set environment
    print("{}:Starting with env={}".format(me,env))

    #WINDOWS
    nick_name = 'uf_local_silodb'
    nick_name = 'uf_local_mysql_marshal1'
    nick_name = 'integration_sobekdb'

    if env == 'windows':
        engine_nick_name = 'uf_local_mysql_marshal1'
        '''
        workbook_path = ('C:\\rvp\\download\\'
          'at_accessions_rvp_20171130.xlsx')

        # todo add a param to test_spreadsheet_table for ann override type per spreadsheet native(but special chars
        # changed) column name. Use a default type for all others
        table_name = 'test_accessions'
        '''

        input_workbook_path = (
          'U:\\data\\ifas_citations\\2016\\base_info\\'
          'IFAS_citations_2016_inspected_20171218a.xls')
        sheet_index = 0

        od_index_column = OrderedDict({
          0: Column('doi',String(256)),
          1: Column('authors',Text),
          2: Column('pub_year', String(32)),
          3: Column('title', Text),
          4: Column('journal', Text),
          5: Column('volume', String(26)),
          6: Column('issue', String(26)),
          7: Column('page_range', String(26)),
          6: Column('original_line', Text),
        })
        table_name = "test_inspected5"
    elif env == 'windows2':
        engine_nick_name = 'uf_local_mysql_marshal1'

        workbook_path = ('C:\\rvp\\download\\'
          'at_accessions_rvp_20171130.xlsx')
        sheet_index = 0
        table_name = 'test_accessions'

        od_index_column = OrderedDict({
          0: Column('col0',String(256)),
          1: Column('c1',Text),
          2: Column('c2', String(32)),
          3: Column('c3', Text),
          4: Column('c4', Text),
          5: Column('c5', String(26)),
          6: Column('c6', String(26)),
          7: Column('c7', String(26)),
          6: Column('c8', Text),
        })

    elif env == 'linux':
        #Linux
        workbook_path = ('/home/robert/git/citrus/projects/lone_cabbage_2017/data/'
          'lone_cabbage_data_janapr_2017.xlsx')
        sheet_index = 0
        engine_nick_name = 'hp_psql'
        table_name = 'test_janapr'
        # Dont really need an ordered dict now, but keep in mind file dumps
        od_index_column = OrderedDict({
          0: Column('county',String(1005),nullable=True),
          1: Column('lake',String(1003),nullable=True),
          2: Column('obs_date',Date,nullable=True),
          6: Column('station_id',Float,nullable=True),
          7: Column('tp_ug_l',Float,nullable=True),
          8: Column('tn_ug_l',Float,nullable=True),
          9: Column('chl_ug_l',Float,nullable=True),
          10: Column('secchi_ft',Float,nullable=True),
          11: Column('secchi_2',String(30)),
          12: Column('color_pt_co_units',Float,nullable=True),
          13: Column('specific_conductance_us_cm_25_c',Float,nullable=True),
          14: Column('specific_conductance_ms_cm_25_c',Float,nullable=True),
        })
    elif env == 'linux3':
        engine_nick_name = 'hp_psql'

        workbook_path = ('/home/robert/git/citrus/projects/archives/data/'
          'at_names_rvp_20171130.xlsx')
        sheet_index = 0
        table_name = 'test_names'

        od_index_column = OrderedDict({
          0: Column('nameid',Integer),
          2: Column('last_updated',Date),
          4: Column('last_updated_by', String(32)),
          6: Column('name_type', Text),
          7: Column('sort_name', Text),
          18: Column('personal_date_range_string', String(26)),
          19: Column('personal_fuller_form', String(26)),
          20: Column('personal_title', Text),
          20: Column('family_name', Text),
          24: Column('name_source', Text),
          29: Column('salutation', Text),
        })


    print(
      "{}:Using env={},\n"
      "workbook_path={},sheet_index={},\n"
      "od_index_column={},\n"
      "engine_nick_name={},table_name={}"
      .format(me,env,workbook_path,sheet_index,
        od_index_column,engine_nick_name,table_name))

    spreadsheet_to_table(
      # Identify the workbook pathname of the input workbook
      input_workbook_path=workbook_path,
      sheet_index=sheet_index,
      # Map the input workbook first spreadsheet's row's column
      # indices to the output table's sqlalchemy columns
      od_index_column=od_index_column,
      #Set the desired output engine/table_name
      engine_nick_name=engine_nick_name,table_name=table_name,
          )

    print("Done!")
    return
#end run()

env = 'windows'
env = 'linux'
env = 'linux2' #implement soon

env = 'linux'
env = 'linux3'

run(env=env)
