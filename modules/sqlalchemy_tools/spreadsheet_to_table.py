'''
(1) For a given excel spreadsheet print the header column names.
'''

import sys, os, os.path, platform
import datetime
from collections import OrderedDict

def register_modules():
    platform_name = platform.system().lower()
    if platform_name == 'linux':
        modules_root = '/home/robert/'
        #raise ValueError("MISSING: Enter code here to define modules_root")
    else:
        # assume rvp office pc running windows
        modules_root="C:\\rvp\\"
    sys.path.append('{}git/citrus/modules'.format(modules_root))
    return
register_modules()

print("sys.path={}".format(repr(sys.path)))

import etl
# Import slate of databases that podengo can use
from sqlalchemy_tools.podengo_db_engine_by_name import get_db_engine_by_name

#### Sqlalchemy
import datetime
from sqlalchemy import (
  Boolean, create_engine,
  CheckConstraint, Column, Date, DateTime, ForeignKeyConstraint,
  inspect, Integer,
  MetaData, Sequence, String, Table, Text, UniqueConstraint,
  )
from sqlalchemy.schema import CreateTable

#
from pathlib import Path
from etl import html_escape, has_digit, has_upper, make_home_relative_folder
import xlrd, xlwt
from xlwt import easyxf
from xlrd import open_workbook
#
from dataset.dataset_code import SheetDictReader, BookSheetFilter

'''
Given a table name and metadata, create a table configuration with:
(1) a primary key column generated by this method
(2) followed by either a list of columns in argument columns
(3) or, if not given, then use column names to create Text columns for
each of the column names

'''
def sqlalchemy_core_table(
  metadata=None, table_name=None, columns=None, column_names=None):
    me = 'sqlalchemy_core_table'
    # create primary key column
    table_columns = [Column(
     '{}_id'.format(table_name), Integer,
      Sequence('{}_id_seq'.format(table_name)), primary_key=True,)]

    if columns is None:
        for c in column_names:
            print("Column name={}",c)
            table_columns.append(Column('{}'.format(c),Text))
    else: #just use columns arg
        table_columns.extend(columns)

    core_table = Table(table_name, MetaData(),*columns);

    print("{}: configured table {}".format(me,core_table.name))
    return core_table

'''
Just print the table creation ddl
'''

def engine_table_create (metadata=None,table_core=None,engine=None,verbosity=1):

    me = "engine_table_create"
    if table_core is None:
      raise ValueError("{}:Got a table_core value of None".format(me))

    if verbosity > 0:
        print('\n{}:-----------------TABLE {}----------------------------\n'
          .format(me,table_core.name))
        print('{}:-----------------ENGINE {}--------------------------\n'
          .format(me,engine.name))

    if verbosity > 0:
        # print the ddl to create the table
        print(CreateTable(table_core).compile(engine))
        print('======================================')

    # Create this table in the engine?
    #table.__table__.create(engine, checkfirst=True)
    engine_table = table_core.create(engine, checkfirst=True)
    # Return None for now...just pringing the schema..
    return engine_table

# end engine_table_create
def workbook_columns(workbook_path=None,sqlalchemy_columns=None):
    #initialize database connections for writing/inserting
    workbook = xlrd.open_workbook(workbook_path)
    first_sheet = workbook.sheet_by_index(0)
    if first_sheet is None:
      raise ValueError("Sheet is None")
    # output
    #reader = SheetDictReader(book=workbook,
    reader = BookSheetFilter(book=workbook,
      sheet=first_sheet, row_count_header=1, row_count_values_start=2,
      sqlalchemy_columns=sqlalchemy_columns,verbosity=0)

    for column_name in reader.column_names:
        column_name = column_name.strip().lower().replace(' ', '_')
        print("{}".format(repr(column_name)))

    return reader.column_names

'''
Connect to a database and insert spreadsheet rows to table

<param name='workbook_path'>
File path to an excel workbook to open, and use the first sheet as the
data source.
</param>
<param name='cxs_format'>
A python string format to use to construct the database connection string,
along with other parameter d_format_params.
</param>
<param name='d_format_params'>
Defines the names and values to use to insert into the cxs_format paramater
string, to use to create a database connection.
</param>
<note>
Used to use next default, but keep here for reference
  cxs_format='mysql+mysqldb://{user}:{password}@127.0.0.1:3306/{dbname}',
 </note>
'''

def spreadsheet_to_engine_table(
  workbook_path=None, table_core=None, engine_table=None, engine=None, od_index_column=None,
  row_count_header=1, row_count_values_start=2, verbosity=0):

    me = 'spreadsheet_to_engine_table'
    required_args = [
      'workbook_path', 'table_core', 'engine', 'od_index_column' ]

    if not all(required_args):
      msg = "Missing some required args in {}".format(repr(required_args))
      raise ValueError(msg)

    #initialize database connections for writing/inserting
    metadata = MetaData(engine)
    inspector = inspect(engine)
    conn = engine.connect()
    metadata.reflect(engine)

    if (verbosity > 0):
        print('Connected with conn={} to database to insert into table {}'
          .format(repr(conn),table.name))
        sys.stdout.flush()

    #workbook
    workbook = xlrd.open_workbook(workbook_path)
    # initialize sheet reader for the workbook

    reader = SheetDictReader(
      book=workbook,sheet_index=0, row_count_header=row_count_header,
      row_count_values_start=row_count_values_start)
    first_sheet = workbook.sheet_by_index(reader.sheet_index)
    print("{}: SheetDictReader with sheet_index {} has column_names={}"
        .format(me, reader.sheet_index, repr(reader.column_names)))
    sys.stdout.flush()

    #Read each spreadsheet row and insert table row
    #based on od_index_column information

    i = 0
    for row in reader:
        i += 1
        if (verbosity > 0 or 1 == 1):
            msg = ("{}:reading row {}={}".format(me,i,repr(row)))
            print(msg.encode('utf-8'))

        od_table_column__value = {}

        # Filter out the interesting table_column value pairs for insertion
        # into the table
        for index, column in od_index_column.items():
            # Create entry in filtered dict with key of each interesting
            # output column name paired with its  row's value
            value = row[reader.column_names[index]]
            od_table_column__value[column.name] = value

            msg = ("row={}, index={}, column_name={}, len={},value={}"
              .format(i,index,column.name,len(value),value))
            # Try to avoid windows msg: UnicodeEncodeError...
            # on prints to windows console, encode in utf-8
            # It works FINE!
            print(msg.encode('utf-8'))
            #print(msg)
            sys.stdout.flush()

        #engine.execute(engine_table.insert(), od_table_column__value)
        engine.execute(table_core.insert(), od_table_column__value)

        if i % 100 == 0:
           print(i)

#end spreadsheet_to_engine_table(workbook_path=None, table=None, engine=None):
'''
Test linux using postgresql example
Required:
A workbook has been deposited at workbook_path and has at least one excel-known
date column

And the posgresql database engine is up and running and the test table does
not exist (just drop that table before running this)

'''
def xxtest_linux(nick_name=None):
    me = 'test_linux_postgres'

    metadata = MetaData()
    engine_nick_name = nick_name
    my_db_engine = get_db_engine_by_name(engine_nick_name)

    workbook_path = ('/home/robert/Downloads/'
        'lone_cabbage_data_janapr_2017.xlsx')

    table_name = "test_table2"
    # Nick name is used by podengo_db_engine_by_bame() to get
    # the desired engine in which to create the table and insert rows.
    # todo: make it drop the table first, or give option to add new
    # rows if table already extant in the engine/database.

    #engine_nick_name = 'local-silodb'
    #engine_nick_name = 'mysql-marshal1'


    print("Calling workbook_columns()....")
    ss_columns = workbook_columns(workbook_path=workbook_path)

    #normalize spreadsheet column names to db table column names
    # Consider moving this 'replace' stuff into SheetDictReader code
    # maybe as a dictionary param or  even hard-coded there .
    # But just do it explicitly here for now
    d_ss_column__table_column = {}
    for ss_column in ss_columns:
        table_column = column_name_normalize(column_name=ss_column)
        d_ss_column__table_column[ss_column] = table_column

    print(
      "{}: using d_ss_column__table_column={}"
      .format(me,repr(d_ss_column__table_column)))


    table=sqlalchemy_core_table(metadata=metadata, table_name=table_name,
        column_names=d_ss_column__table_column.values(), )

    # select a db engine
    # get_db_engine_by_name is a custom function per user who runs this
    # test for now, with that users creds on linux or windows if windows
    # authentication is not being used on the windows database

    tables = [table]
    # todo:Change to arg of single table instead of list tables
    # create the table if not extand
    creates_run(metadata=metadata,engine=my_db_engine,tables=tables)

    #Add rows to the table from the spreadsheet
    spreadsheet_to_engine_table(workbook_path=workbook_path, table=table,
       engine=my_db_engine,od_index_column=od_index_column)

    return
# end test_linux

def ss_column_name_normalize(column_name=None):
  column = (
           column.replace('/','_').replace('-','_')
          .replace('(','_').replace(')','')
          .replace(u'\u00B5','u') #micro sign
          .replace(u'\u03BC','u') #greek mu
          .replace(u'\u0040','') # commercial at
          .replace(u'\uFF20','') # fullwidth commercial at
          .replace(u'\uFE6B','') # small commercial at
          .replace(u'\u00B0','') # degrees symbol
          )
  return column


'''
Set workbook_path to any workbook path on local drive

Required: the test workbook is at workbook_path, defined below.
'''
def test_spreadsheet_table(
  input_workbook_path=None ,od_index_column=None
  ,engine_nick_name=None, table_name=None
  ):

    me = 'test_spreadsheet_table'
    required_args =[
      'input_workbook_path',
      'od_index_column',
      'engine_nick_name',
      'table_name',
    ]
    if not all(required_args):
      msg=("Mising some required args: {}".format(repr(required_args)))
      raise ValueError(msg)

    metadata = MetaData()
    my_db_engine = get_db_engine_by_name(name=engine_nick_name)

    #Create the in-memory table_core object.

    table_core = sqlalchemy_core_table(
      table_name=table_name, columns=od_index_column.values())

    # todo: only show the create ddl for the table
    # TODO:From the table_core, create a true database table object

    engine_table = engine_table_create(
      metadata=metadata, engine=my_db_engine, table_core=table_core)

    #Add rows to the table from the spreadsheet

    spreadsheet_to_engine_table(
       workbook_path=input_workbook_path, engine_table=engine_table,
       table_core=table_core,
       engine=my_db_engine, od_index_column=od_index_column )

    return
# end test_windows()

def run(env=None):
    me='run()'

    #NOTE set environment
    print("{}:Starting with env={}".format(me,env))

    #WINDOWS
    nick_name = 'uf_local_silodb'
    nick_name = 'uf_local_mysql_marshal1'
    nick_name = 'integration_sobekdb'

    if env == 'windows':
        engine_nick_name = 'uf_local_mysql_marshal1'
        print(
          "{}:Using env={}, engine_nick_name={}"
          .format(me,env,engine_nick_name))

        '''
        workbook_path = ('C:\\rvp\\download\\'
          'at_accessions_rvp_20171130.xlsx')

        # todo add a param to test_spreadsheet_table for ann override type per spreadsheet native(but special chars
        # changed) column name. Use a default type for all others
        table_name = 'test_accessions'
        '''

        input_workbook_path = (
          'U:\\data\\ifas_citations\\2016\\base_info\\'
          'IFAS_citations_2016_inspected_20171218a.xls')

        od_index_column = OrderedDict({
          5: Column('abc',String(1005)),
          3: Column('def',String(1003))
        })
        table_name = "test_inspected5"

        test_spreadsheet_table(
          input_workbook_path=input_workbook_path,
          od_index_column=od_index_column,
          engine_nick_name=engine_nick_name,
          table_name=table_name,
         )

    elif env == 'linux':
        #Linux
        engine_nick_name = 'hp_psql'
        print(
          "{}:Using env={}, engine_nick_name={}"
          .format(me,env,engine_nick_name))

        workbook_path = ('/home/robert/git/citrus/projects/lone_cabbage_2017/data/'
          'lone_cabbage_data_janapr_2017.xlsx')
        # Dont really need an ordered dict now, but keep in mind file dumps
        od_index_column = OrderedDict({
          5: Column('abc',String(1005)),
          3: Column('def',String(1003))
        })
        table_name = 'test_janapr'

        test_spreadsheet_table(
          # Identify the workbook pathname of the input workbook
          input_workbook_path=workbook_path,
          # Map the input workbook first spreadsheet's row's column
          # indices to the output table's sqlalchemy columns
          od_index_column=od_index_column,
          #Set the desired output engine/table_name
          engine_nick_name=engine_nick_name,table_name=table_name,
          )

    print("Done!")
    return
#end run()

env = 'windows'
env = 'linux'

env = 'windows'

run(env=env)
